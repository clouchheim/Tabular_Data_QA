{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064226fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, TapasConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468c9453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4fae061e7b461980ea50efba1c329c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9430a94097b944abb21a51fe186a5659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d972a43d92ad45e89825b481db8a46f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d1b2d149174b70bc72cea9cce6423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in all qa (train and dev)\n",
    "semeval_train_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"train\")\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68e0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all of the train datasets\n",
    "dfs_train = list(set(semeval_train_qa['dataset']))\n",
    "dfs_train = sorted(dfs_train, key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# get the names of all of the dev datasets\n",
    "dfs_dev = list(set(semeval_dev_qa['dataset']))\n",
    "dfs_dev = sorted(dfs_dev, key=lambda x: int(x.split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c7444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  001_Forbes\n",
      "CSV for ID 001_Forbes already exists. Skipping...\n",
      "Processing:  002_Titanic\n",
      "CSV for ID 002_Titanic already exists. Skipping...\n",
      "Processing:  003_Love\n",
      "CSV for ID 003_Love already exists. Skipping...\n",
      "Processing:  004_Taxi\n",
      "CSV for ID 004_Taxi already exists. Skipping...\n",
      "Processing:  005_NYC\n",
      "CSV for ID 005_NYC already exists. Skipping...\n",
      "Processing:  006_London\n",
      "CSV for ID 006_London already exists. Skipping...\n",
      "Processing:  007_Fifa\n",
      "CSV for ID 007_Fifa already exists. Skipping...\n",
      "Processing:  008_Tornados\n",
      "CSV for ID 008_Tornados already exists. Skipping...\n",
      "Processing:  009_Central\n",
      "CSV for ID 009_Central already exists. Skipping...\n",
      "Processing:  010_ECommerce\n",
      "CSV for ID 010_ECommerce already exists. Skipping...\n",
      "Processing:  011_SF\n",
      "CSV for ID 011_SF already exists. Skipping...\n",
      "Processing:  012_Heart\n",
      "CSV for ID 012_Heart already exists. Skipping...\n",
      "Processing:  013_Roller\n",
      "CSV for ID 013_Roller already exists. Skipping...\n",
      "Processing:  014_Airbnb\n",
      "CSV for ID 014_Airbnb already exists. Skipping...\n",
      "Processing:  015_Food\n",
      "CSV for ID 015_Food already exists. Skipping...\n",
      "Processing:  016_Holiday\n",
      "CSV for ID 016_Holiday already exists. Skipping...\n",
      "Processing:  017_Hacker\n",
      "CSV for ID 017_Hacker already exists. Skipping...\n",
      "Processing:  018_Staff\n",
      "CSV for ID 018_Staff already exists. Skipping...\n",
      "Processing:  019_Aircraft\n",
      "CSV for ID 019_Aircraft already exists. Skipping...\n",
      "Processing:  020_Real\n",
      "CSV for ID 020_Real already exists. Skipping...\n",
      "Processing:  021_Telco\n",
      "CSV for ID 021_Telco already exists. Skipping...\n",
      "Processing:  022_Airbnbs\n",
      "CSV for ID 022_Airbnbs already exists. Skipping...\n",
      "Processing:  023_Climate\n",
      "CSV for ID 023_Climate already exists. Skipping...\n",
      "Processing:  024_Salary\n",
      "CSV for ID 024_Salary already exists. Skipping...\n",
      "Processing:  025_Data\n",
      "CSV for ID 025_Data already exists. Skipping...\n",
      "Processing:  026_Predicting\n",
      "CSV for ID 026_Predicting already exists. Skipping...\n",
      "Processing:  027_Supermarket\n",
      "CSV for ID 027_Supermarket already exists. Skipping...\n",
      "Processing:  028_Predict\n",
      "CSV for ID 028_Predict already exists. Skipping...\n",
      "Processing:  029_NYTimes\n",
      "CSV for ID 029_NYTimes already exists. Skipping...\n",
      "Processing:  030_Professionals\n",
      "CSV for ID 030_Professionals already exists. Skipping...\n",
      "Processing:  031_Trustpilot\n",
      "CSV for ID 031_Trustpilot already exists. Skipping...\n",
      "Processing:  032_Delicatessen\n",
      "CSV for ID 032_Delicatessen already exists. Skipping...\n",
      "Processing:  033_Employee\n",
      "CSV for ID 033_Employee already exists. Skipping...\n",
      "Processing:  034_World\n",
      "CSV for ID 034_World already exists. Skipping...\n",
      "Processing:  035_Billboard\n",
      "CSV for ID 035_Billboard already exists. Skipping...\n",
      "Processing:  036_US\n",
      "CSV for ID 036_US already exists. Skipping...\n",
      "Processing:  037_Ted\n",
      "CSV for ID 037_Ted already exists. Skipping...\n",
      "Processing:  038_Stroke\n",
      "CSV for ID 038_Stroke already exists. Skipping...\n",
      "Processing:  039_Happy\n",
      "CSV for ID 039_Happy already exists. Skipping...\n",
      "Processing:  040_Speed\n",
      "CSV for ID 040_Speed already exists. Skipping...\n",
      "Processing:  041_Airline\n",
      "CSV for ID 041_Airline already exists. Skipping...\n",
      "Processing:  042_Predict\n",
      "CSV for ID 042_Predict already exists. Skipping...\n",
      "Processing:  043_Predict\n",
      "CSV for ID 043_Predict already exists. Skipping...\n",
      "Processing:  044_IMDb\n",
      "CSV for ID 044_IMDb already exists. Skipping...\n",
      "Processing:  045_Predict\n",
      "CSV for ID 045_Predict already exists. Skipping...\n",
      "Processing:  046_120\n",
      "CSV for ID 046_120 already exists. Skipping...\n",
      "Processing:  047_Bank\n",
      "CSV for ID 047_Bank already exists. Skipping...\n",
      "Processing:  048_Data\n",
      "CSV for ID 048_Data already exists. Skipping...\n",
      "Processing:  049_Boris\n",
      "CSV for ID 049_Boris already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "qa_dict = {} # dict to store all qa \n",
    "output_folder = os.getcwd()\n",
    "for table in dfs_train:\n",
    "    print('Processing: ', table)\n",
    "    csv_file_path = os.path.join(output_folder, f\"{table}.csv\")\n",
    "    \n",
    "    # Load the qa.parquet dataframe and store it in the dictionary\n",
    "    qa = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/qa.parquet\")\n",
    "    qa_dict[table] = qa\n",
    "        \n",
    "    # Skip if the CSV file already exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"CSV for ID {table} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load the all.parquet dataframe and save it as CSV\n",
    "        df = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/sample.parquet\") # loading in the lite versions with only 20 rows\n",
    "        df.to_csv(csv_file_path, index=False)  #### RERUN THIS WHEN I DO THE REAL THING\n",
    "        print(f\"Saved CSV for ID {table} at {csv_file_path}.\")\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3143d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['answer'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m qa \u001b[38;5;241m=\u001b[39m qa_dict[df] \n\u001b[1;32m     10\u001b[0m qa \u001b[38;5;241m=\u001b[39m qa[qa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m---> 11\u001b[0m qa \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m qa \u001b[38;5;241m=\u001b[39m qa\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m~\u001b[39mqa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     13\u001b[0m qa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5589\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['answer'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# filter to only number and category answers for all qa dfs in qa_dict\n",
    "def extract_float(answer):\n",
    "    try:\n",
    "        return float(answer)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "for df in qa_dict:\n",
    "    qa = qa_dict[df] \n",
    "    qa = qa[qa['type'].isin(['number', 'category'])]\n",
    "    qa = qa.drop('answer', axis = 1)\n",
    "    qa = qa.loc[~qa['sample_answer'].isin(['0', 'None'])]\n",
    "    qa['dataset'] = qa['dataset'] + '.csv'\n",
    "    qa['float_answer'] = qa['sample_answer'].apply(extract_float)\n",
    "    qa_dict[df] = qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other questions to remove, make sure you only run this once\n",
    "qa_dict[dfs_train[0]] = qa_dict[dfs_train[0]].iloc[:-1]\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].reset_index(drop=True)\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].drop([2,3,6,7])\n",
    "qa_dict[dfs_train[6]] = qa_dict[dfs_train[6]].reset_index(drop=True).drop([0])\n",
    "qa_dict[dfs_train[12]] = qa_dict[dfs_train[12]].reset_index(drop=True).drop([3])\n",
    "qa_dict[dfs_train[14]] = qa_dict[dfs_train[14]].reset_index(drop=True).drop([0,4,5,7])\n",
    "qa_dict[dfs_train[15]] = qa_dict[dfs_train[15]].reset_index(drop=True).drop([0,4,5,6,7])\n",
    "qa_dict[dfs_train[17]] = qa_dict[dfs_train[17]].reset_index(drop=True).drop([6])\n",
    "qa_dict[dfs_train[19]] = qa_dict[dfs_train[19]].reset_index(drop=True).drop([1])\n",
    "qa_dict[dfs_train[20]] = qa_dict[dfs_train[20]].reset_index(drop=True).drop([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df87db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 019_Aircraft (changed)\n",
    "qa_dict[dfs_train[18]]['answer_coords'] = [\n",
    "    [(0,4),(1,4),(2,4),(3,4),(4,4),(5,4),(6,4),(7,4),(8,4),(9,4),(10,4),(11,4),(12,4),(13,4),(14,4),(15,4),(16,4),(17,4),(18,4),(19,4)],\n",
    "    [(16,3)], \n",
    "    [(0,8),(1,8),(9,8),(10,8),(11,8),(12,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,4)],\n",
    "    [(9,11)], \n",
    "    [(3,6),(5,6),(7,6),(16,6),(15,6),(14,6)], #(3,6)\n",
    "    [(16,0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 018_Staff (changed)\n",
    "qa_dict[dfs_train[17]]['answer_coords'] = [\n",
    "    [(0,1),(1,1),(2,1),(3,1),(5,1),(9,1),(11,1),(12,1),(18,1)], \n",
    "    [(9,6)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,1)],\n",
    "    [(1,7),(2,7),(3,7),(4,7),(5,7),(8,7),(9,7),(11,7),(12,7),(13,7),(14,7),(17,7),(18,7)], #(1,7)\n",
    "    [(1,10)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 017_Hacker (changed)\n",
    "qa_dict[dfs_train[16]]['answer_coords'] = [\n",
    "    [(3,3)], \n",
    "    [(12,2)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,9),(2,9),(4,9),(11,9),(14,9),(15,9),(16,9),(18,9)],\n",
    "    [(1,6),(2,6),(9,6),(10,6),(13,6),(15,6)], # (1,6)\n",
    "    [(0,5),(1,5),(2,5),(3,5),(5,5),(6,5),(7,5),(8,5),(9,5),(10,5),(11,5),(13,5),(14,5),(16,5),(17,5),(18,5),(19,5)], # (0,5)\n",
    "    [(12,9)],\n",
    "    [(0,3),(2,3),(4,3),(5,3),(8,3),(9,3),(10,3),(11,3),(12,3),(13,3),(14,3),(15,3),(16,3)] #(0,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86079e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 016_Holiday (no changes needed)\n",
    "qa_dict[dfs_train[15]]['answer_coords'] = [\n",
    "    [(0,6),(4,6),(5,6)], \n",
    "    [(0,18),(4,18),(5,18),(10,18)], \n",
    "    [(0,12),(3,12),(9,12),(10,12)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 015_Food (no changes needed)\n",
    "qa_dict[dfs_train[14]]['answer_coords'] = [\n",
    "    [(0,2),(1,2),(2,2),(3,2),(7,2),(8,2),(9,2),(18,2)], \n",
    "    [(0,3),(1,3),(2,3),(3,3),(5,3),(6,3),(7,3),(8,3),(9,3),(12,3),(15,3),(17,3),(18,3),(19,3)],   \n",
    "    [(1,0),(2,0),(3,0),(4,0),(5,0),(6,0),(7,0),(8,0),(9,0),(10,0),(11,0),(12,0),(13,0),(14,0),(15,0),(16,0),(17,0),(18,0),(19,0),(0,0)], \n",
    "    [(1,1)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 013_Roller (no changes needed)\n",
    "qa_dict[dfs_train[12]]['answer_coords'] = [\n",
    "    [(12,8)], \n",
    "    [(3,5),(4,5),(9,5),(10,5),(11,5)], \n",
    "    [(12,3)],   \n",
    "    [(1,6)], \n",
    "    [(15,10)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 012_Heart (changed)\n",
    "qa_dict[dfs_train[11]]['answer_coords'] = [\n",
    "    [(7,1)], \n",
    "    [(17,2)],\n",
    "    [(0,0),(1,0),(4,0),(5,0),(7,0),(8,0),(10,0),(11,0),(12,0),(13,0),(14,0),(16,0),(18,0),(19,0)], \n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7),(8,7),(9,7),(10,7),(11,7),(12,7),(13,7),(14,7),(15,7),(16,7),(17,7),(18,7),(19,7)],  \n",
    "    [(2,5),(3,5),(5,5),(6,5),(9,5),(12,5),(14,5),(15,5),(17,5),(18,5)], #(2,5)\n",
    "    [(2,6),(6,6)], #(2,6)\n",
    "    [(1,4),(2,4),(3,4),(5,4),(6,4),(8,4),(10,4),(12,4),(14,4),(18,4),(19,4)], #(1,4)\n",
    "    [(8,6),(10,6)] #(8,6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a83b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 011_SF (changes made)\n",
    "qa_dict[dfs_train[10]]['answer_coords'] = [\n",
    "    [(0,1),(2,1),(7,1),(8,1),(9,1),(10,1),(11,1),(13,1),(16,1),(18,1),(19,1)], \n",
    "    [(5,9)],\n",
    "    [(0,7),(1,7),(3,7),(4,7),(5,7),(8,7),(9,7),(10,7),(14,7)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)], # this might not work  \n",
    "    [(2,1),(4,1),(5,1),(5,1),(15,1)], #(2,1)\n",
    "    [(5,4),(8,4),(14,4),(17,4),(19,4)], #(5,4)\n",
    "    [(5,6)],\n",
    "    [(4,7),(6,7),(17,7),(16,7)] #(4,7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8273cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 010_ECommerce (changed)\n",
    "qa_dict[dfs_train[9]]['answer_coords'] = [\n",
    "    [(0,2),(1,2),(2,2),(3,2),(4,2),(5,2),(6,2),(7,2),(8,2),(9,2),(10,2),(11,2),(12,2),(13,2),(14,2),(15,2),(16,2),(17,2),(18,2),(19,2)], \n",
    "    [(0,0)], \n",
    "    [(4,5),(5,5),(7,5),(8,5),(9,5),(10,5),(13,5),(15,5),(17,5),(19,5)], # (4,5)\n",
    "    [(0,1),(1,1),(2,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1),(10,1),(11,1),(12,1),(13,1),(14,1),(15,1),(16,1),(17,1),(18,1),(19,1),],  \n",
    "    [(0,3),(4,3),(7,3),(10,3),(12,3),(13,3),(19,3)], # (0,3)\n",
    "    [(0,5),(4,5),(7,5),(10,5),(12,5),(13,5),(19,5)], # (0,5)\n",
    "    [(0,6),(2,6),(4,6),(7,6),(8,6),(12,6),(14,6),(15,6),(17,6),(18,6)], #(0,6)\n",
    "    [(0,1)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a464a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 009_Central (not changes needed)\n",
    "qa_dict[dfs_train[8]]['answer_coords'] = [\n",
    "    [(2,1)], \n",
    "    [(6,0)], \n",
    "    [(2,4)], \n",
    "    [(1,3)], \n",
    "    [(2,2)], \n",
    "    [(6,2)], \n",
    "    [(2,2)],\n",
    "    [(1,2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d20339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 008_Tornadoes (changed)\n",
    "qa_dict[dfs_train[7]]['answer_coords'] = [\n",
    "    [(0,6),(1,6),(2,6),(3,6),(4,6),(7,6),(8,6),(9,6),(12,6),(15,6),(16,6),(18,6)], \n",
    "    [(10,3)],\n",
    "    [(11,7)],\n",
    "    [(16,4)],\n",
    "    [(4,6),(5,6),(6,6),(17,6)], # (4,6)\n",
    "    [(4,5),(6,5),(7,5),(11,5),(18,5)], #(6,5)\n",
    "    [(16,0)],\n",
    "    [(11,0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00383700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 007_Fifa (changed)\n",
    "qa_dict[dfs_train[6]]['answer_coords'] = [\n",
    "    [(0,11),(1,11),(2,11),(3,11),(4,11),(5,11),(6,11),(7,11),(8,11),(9,11),(10,11),(11,11),(12,11),(13,11),(14,11),(15,11),(16,11),(17,11),(18,11)], \n",
    "    [(4,10)],\n",
    "    [(11,3)],\n",
    "    [(0,8),(2,8)], #(0,8)\n",
    "    [(0,5),(1,5),(2,5),(4,5),(5,5),(6,5),(7,5),(8,5),(9,5),(10,5),(11,5),(12,5),(13,5),(16,5),(17,5)], #(0,5)\n",
    "    [(12,11),(19,11)], #(19,11)\n",
    "    [(0,3),(2,3),(4,3),(8,3),(9,3),(10,3),(12,3),(13,3),(14,3),(15,3)] #(2,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 005_NYC (changed)\n",
    "qa_dict[dfs_train[4]]['answer_coords'] = [\n",
    "    [(0,5),(1,5),(3,5),(4,5),(5,5),(11,5),(15,5)], \n",
    "    [(14,6)], # dont get\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(8,7),(9,7),(10,7),(11,7),(13,7),(14,7),(15,7),(18,7),(19,7)],\n",
    "    [(1,1),(3,1),(7,1),(9,1),(11,1),(12,1),(13,1),(14,1),(19,1)], #(1,1)\n",
    "    [(4,3),(5,3),(8,3),(9,3)], #(4,3)\n",
    "    [(17,4)],\n",
    "    [(4,5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 004_Taxi (changes made)\n",
    "qa_dict[dfs_train[3]]['answer_coords'] = [\n",
    "    [(2,3)], #\n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(15,8),(16,8),(18,8),(19,8)],\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7),(8,7),(9,7),(10,7),(11,7),(12,7),(13,7),(14,7),(15,7),(16,7),(17,7),(18,7),(19,7)],\n",
    "    [(0,1),(1,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1),(11,1),(12,1),(13,1),(15,1),(16,1),(17,1),(18,1),(19,1)], #(0,1)\n",
    "    [(1,4),(2,4),(3,4),(4,4),(6,4),(7,4),(8,4),(9,4),(10,4),(12,4),(13,4),(17,4)], # (1,4)\n",
    "    [(3,5),(4,5),(9,5)], #(3,5)\n",
    "    [(0,2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e57e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 002_Titanic (changes made)\n",
    "qa_dict[dfs_train[1]]['answer_coords'] = [\n",
    "    [(0, 4)],\n",
    "    [(0, 0)],\n",
    "    [(0, 4),(2, 4),(3, 4),(5, 4),(6, 4),(8, 4),(12, 4),(14, 4),(15, 4),(16, 4),(18, 4)], # (0, 4)\n",
    "    [(12, 2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32681f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 001_Forbes (not changes needed)\n",
    "qa_dict[dfs_train[0]]['answer_coords'] = [\n",
    "    [(11,5)], \n",
    "    [(14,6)],\n",
    "    [(7,8)],\n",
    "    [(18,10)],\n",
    "    [(0,4)],\n",
    "    [(11,9)] \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c85d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up the WTQ style tokenizer for testing if will tokenize\n",
    "config = TapasConfig.from_pretrained(\n",
    "    \"google/tapas-base-finetuned-wtq\",\n",
    "    aggregation_labels=True,  # Enable aggregation operators\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer and model with the configuration\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be80e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_ids = [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e7ab444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error tokenizing  001_Forbes\n",
      "002_Titanic  tokenized with no error\n",
      "error tokenizing  004_Taxi\n",
      "error tokenizing  005_NYC\n",
      "error tokenizing  007_Fifa\n",
      "error tokenizing  008_Tornados\n",
      "error tokenizing  009_Central\n",
      "error tokenizing  010_ECommerce\n",
      "error tokenizing  011_SF\n",
      "error tokenizing  012_Heart\n",
      "error tokenizing  013_Roller\n",
      "error tokenizing  015_Food\n",
      "error tokenizing  016_Holiday\n",
      "error tokenizing  017_Hacker\n",
      "error tokenizing  018_Staff\n",
      "error tokenizing  019_Aircraft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:2673: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1472: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    }
   ],
   "source": [
    "for df_num in working_ids:\n",
    "    try:\n",
    "        table = pd.read_csv(qa_dict[dfs_train[df_num]]['dataset'][0]).astype(str)\n",
    "        queries = list(qa_dict[dfs_train[df_num]]['question'])\n",
    "        answer_coordinates = list(qa_dict[dfs_train[df_num]]['answer_coords'])\n",
    "        answer_text = list(qa_dict[dfs_train[df_num]]['sample_answer'])\n",
    "        inputs = tokenizer(\n",
    "            table = table,\n",
    "            queries = queries,\n",
    "            answer_coordinates = answer_coordinates,\n",
    "            answer_text = answer_text,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,  \n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "        print(dfs_train[df_num], ' tokenized with no error')\n",
    "    except:\n",
    "        print('error tokenizing ', dfs_train[df_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4683e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets in the working_ids to a dataset\n",
    "working_df = pd.DataFrame()\n",
    "for i in working_ids:\n",
    "    df = qa_dict[dfs_train[i]]\n",
    "    working_df = pd.concat([working_df, df], ignore_index=True)\n",
    "    \n",
    "working_df.to_csv('toy_df_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b308094",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids = [40,41,42,42,44,45]\n",
    "for i in dev_ids:\n",
    "    df = qa_dict[dfs_train[i]]\n",
    "    working_df = pd.concat([working_df, df], ignore_index=True)\n",
    "    \n",
    "working_df.to_csv('dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7594bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [34,35,36,37,38,39]\n",
    "for i in dev_ids:\n",
    "    df = qa_dict[dfs_train[i]]\n",
    "    working_df = pd.concat([working_df, df], ignore_index=True)\n",
    "    \n",
    "working_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff31f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
