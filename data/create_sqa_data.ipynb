{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a836e767",
   "metadata": {},
   "source": [
    "We need to format our data into SQA format and save into a csv/tsv for the finetuning which needs:\n",
    "\n",
    "id: optional, id of the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "annotator: optional, id of the person who annotated the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "position: integer indicating if the question is the first, second, third,… related to the table. Only required in case of conversational setup (SQA). You don’t need this column in case you’re going for WTQ/WikiSQL-supervised.\n",
    "\n",
    "question: string\n",
    "\n",
    "table_file: string, name of a csv file containing the tabular data\n",
    "answer_coordinates: list of one or more tuples (each tuple being a cell coordinate, i.e. row, column pair that is part of the answer)\n",
    "\n",
    "answer_text: list of one or more strings (each string being a cell value that is part of the answer)\n",
    "aggregation_label: index of the aggregation operator. Only required in case of strong supervision for aggregation (the WikiSQL-supervised case)\n",
    "\n",
    "float_answer: the float answer to the question, if there is one (np.nan if there isn’t). Only required in case of weak supervision for aggregation (such as WTQ and WikiSQL)\n",
    "\n",
    "the tables refered to in the table_file area should be saved in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2c28430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, TapasConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d509af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb95b70b2574c20b84c764f28e6e774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8712cb13880941219d6fdd4b1185a7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8a269100244a0eb735d8805603661e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2a75809fc946ccbf98af683bc29b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in all qa (train and dev)\n",
    "semeval_train_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"train\")\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2d2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all of the train datasets\n",
    "dfs_train = list(set(semeval_train_qa['dataset']))\n",
    "dfs_train = sorted(dfs_train, key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# get the names of all of the dev datasets\n",
    "dfs_dev = list(set(semeval_dev_qa['dataset']))\n",
    "dfs_dev = sorted(dfs_dev, key=lambda x: int(x.split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e14f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  001_Forbes\n",
      "CSV for ID 001_Forbes already exists. Skipping...\n",
      "Processing:  002_Titanic\n",
      "CSV for ID 002_Titanic already exists. Skipping...\n",
      "Processing:  003_Love\n",
      "CSV for ID 003_Love already exists. Skipping...\n",
      "Processing:  004_Taxi\n",
      "CSV for ID 004_Taxi already exists. Skipping...\n",
      "Processing:  005_NYC\n",
      "CSV for ID 005_NYC already exists. Skipping...\n",
      "Processing:  006_London\n",
      "CSV for ID 006_London already exists. Skipping...\n",
      "Processing:  007_Fifa\n",
      "CSV for ID 007_Fifa already exists. Skipping...\n",
      "Processing:  008_Tornados\n",
      "CSV for ID 008_Tornados already exists. Skipping...\n",
      "Processing:  009_Central\n",
      "CSV for ID 009_Central already exists. Skipping...\n",
      "Processing:  010_ECommerce\n",
      "CSV for ID 010_ECommerce already exists. Skipping...\n",
      "Processing:  011_SF\n",
      "CSV for ID 011_SF already exists. Skipping...\n",
      "Processing:  012_Heart\n",
      "CSV for ID 012_Heart already exists. Skipping...\n",
      "Processing:  013_Roller\n",
      "CSV for ID 013_Roller already exists. Skipping...\n",
      "Processing:  014_Airbnb\n",
      "CSV for ID 014_Airbnb already exists. Skipping...\n",
      "Processing:  015_Food\n",
      "CSV for ID 015_Food already exists. Skipping...\n",
      "Processing:  016_Holiday\n",
      "CSV for ID 016_Holiday already exists. Skipping...\n",
      "Processing:  017_Hacker\n",
      "CSV for ID 017_Hacker already exists. Skipping...\n",
      "Processing:  018_Staff\n",
      "CSV for ID 018_Staff already exists. Skipping...\n",
      "Processing:  019_Aircraft\n",
      "CSV for ID 019_Aircraft already exists. Skipping...\n",
      "Processing:  020_Real\n",
      "CSV for ID 020_Real already exists. Skipping...\n",
      "Processing:  021_Telco\n",
      "CSV for ID 021_Telco already exists. Skipping...\n",
      "Processing:  022_Airbnbs\n",
      "CSV for ID 022_Airbnbs already exists. Skipping...\n",
      "Processing:  023_Climate\n",
      "CSV for ID 023_Climate already exists. Skipping...\n",
      "Processing:  024_Salary\n",
      "CSV for ID 024_Salary already exists. Skipping...\n",
      "Processing:  025_Data\n",
      "CSV for ID 025_Data already exists. Skipping...\n",
      "Processing:  026_Predicting\n",
      "CSV for ID 026_Predicting already exists. Skipping...\n",
      "Processing:  027_Supermarket\n",
      "CSV for ID 027_Supermarket already exists. Skipping...\n",
      "Processing:  028_Predict\n",
      "CSV for ID 028_Predict already exists. Skipping...\n",
      "Processing:  029_NYTimes\n",
      "CSV for ID 029_NYTimes already exists. Skipping...\n",
      "Processing:  030_Professionals\n",
      "CSV for ID 030_Professionals already exists. Skipping...\n",
      "Processing:  031_Trustpilot\n",
      "CSV for ID 031_Trustpilot already exists. Skipping...\n",
      "Processing:  032_Delicatessen\n",
      "CSV for ID 032_Delicatessen already exists. Skipping...\n",
      "Processing:  033_Employee\n",
      "CSV for ID 033_Employee already exists. Skipping...\n",
      "Processing:  034_World\n",
      "CSV for ID 034_World already exists. Skipping...\n",
      "Processing:  035_Billboard\n",
      "CSV for ID 035_Billboard already exists. Skipping...\n",
      "Processing:  036_US\n",
      "CSV for ID 036_US already exists. Skipping...\n",
      "Processing:  037_Ted\n",
      "CSV for ID 037_Ted already exists. Skipping...\n",
      "Processing:  038_Stroke\n",
      "CSV for ID 038_Stroke already exists. Skipping...\n",
      "Processing:  039_Happy\n",
      "CSV for ID 039_Happy already exists. Skipping...\n",
      "Processing:  040_Speed\n",
      "CSV for ID 040_Speed already exists. Skipping...\n",
      "Processing:  041_Airline\n",
      "CSV for ID 041_Airline already exists. Skipping...\n",
      "Processing:  042_Predict\n",
      "CSV for ID 042_Predict already exists. Skipping...\n",
      "Processing:  043_Predict\n",
      "CSV for ID 043_Predict already exists. Skipping...\n",
      "Processing:  044_IMDb\n",
      "CSV for ID 044_IMDb already exists. Skipping...\n",
      "Processing:  045_Predict\n",
      "CSV for ID 045_Predict already exists. Skipping...\n",
      "Processing:  046_120\n",
      "CSV for ID 046_120 already exists. Skipping...\n",
      "Processing:  047_Bank\n",
      "CSV for ID 047_Bank already exists. Skipping...\n",
      "Processing:  048_Data\n",
      "CSV for ID 048_Data already exists. Skipping...\n",
      "Processing:  049_Boris\n",
      "CSV for ID 049_Boris already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "qa_dict = {} # dict to store all qa \n",
    "output_folder = os.getcwd()\n",
    "for table in dfs_train:\n",
    "    print('Processing: ', table)\n",
    "    csv_file_path = os.path.join(output_folder, f\"{table}.csv\")\n",
    "    \n",
    "    # Load the qa.parquet dataframe and store it in the dictionary\n",
    "    qa = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/qa.parquet\")\n",
    "    qa_dict[table] = qa\n",
    "        \n",
    "    # Skip if the CSV file already exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"CSV for ID {table} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load the all.parquet dataframe and save it as CSV\n",
    "        df = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/sample.parquet\") # loading in the lite versions with only 20 rows\n",
    "        df.to_csv(csv_file_path, index=False)  #### RERUN THIS WHEN I DO THE REAL THING\n",
    "        print(f\"Saved CSV for ID {table} at {csv_file_path}.\")\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48667efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['001_Forbes', '002_Titanic', '003_Love', '004_Taxi', '005_NYC', '006_London', '007_Fifa', '008_Tornados', '009_Central', '010_ECommerce', '011_SF', '012_Heart', '013_Roller', '014_Airbnb', '015_Food', '016_Holiday', '017_Hacker', '018_Staff', '019_Aircraft', '020_Real', '021_Telco', '022_Airbnbs', '023_Climate', '024_Salary', '025_Data', '026_Predicting', '027_Supermarket', '028_Predict', '029_NYTimes', '030_Professionals', '031_Trustpilot', '032_Delicatessen', '033_Employee', '034_World', '035_Billboard', '036_US', '037_Ted', '038_Stroke', '039_Happy', '040_Speed', '041_Airline', '042_Predict', '043_Predict', '044_IMDb', '045_Predict', '046_120', '047_Bank', '048_Data', '049_Boris'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign all of the qa tables\n",
    "# for each need to manually assing the answer coordinate to each qa row\n",
    "qa_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77981439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only number and category answers for all qa dfs in qa_dict\n",
    "def extract_float(answer):\n",
    "    try:\n",
    "        return float(answer)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "for df in qa_dict:\n",
    "    qa = qa_dict[df] \n",
    "    qa = qa[qa['type'].isin(['number', 'category'])]\n",
    "    qa = qa.drop('answer', axis = 1)\n",
    "    qa = qa.loc[~qa['sample_answer'].isin(['0', 'None'])]\n",
    "    qa['dataset'] = qa['dataset'] + '.csv'\n",
    "    qa['float_answer'] = qa['sample_answer'].apply(extract_float)\n",
    "    qa_dict[df] = qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cdc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other questions to remove, make sure you only run this once\n",
    "qa_dict[dfs_train[0]] = qa_dict[dfs_train[0]].iloc[:-1]\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].reset_index(drop=True)\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].drop([2,3,6,7])\n",
    "qa_dict[dfs_train[6]] = qa_dict[dfs_train[6]].reset_index(drop=True).drop([0])\n",
    "qa_dict[dfs_train[12]] = qa_dict[dfs_train[12]].reset_index(drop=True).drop([3])\n",
    "qa_dict[dfs_train[14]] = qa_dict[dfs_train[14]].reset_index(drop=True).drop([0,4,5,7])\n",
    "qa_dict[dfs_train[15]] = qa_dict[dfs_train[15]].reset_index(drop=True).drop([0,4,5,6,7])\n",
    "qa_dict[dfs_train[17]] = qa_dict[dfs_train[17]].reset_index(drop=True).drop([6])\n",
    "qa_dict[dfs_train[19]] = qa_dict[dfs_train[19]].reset_index(drop=True).drop([1])\n",
    "qa_dict[dfs_train[20]] = qa_dict[dfs_train[20]].reset_index(drop=True).drop([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "39d946af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "019_Aircraft\n",
      "How many unique aircraft models are in the dataset?\n",
      "     ---> 20\n",
      "\n",
      "\n",
      "What's the highest number of occupants recorded in an incident?\n",
      "     ---> 86\n",
      "\n",
      "\n",
      "How many incidents resulted in non-zero fatalities?\n",
      "     ---> 10\n",
      "\n",
      "\n",
      "Which aircraft model was involved in the most incidents?\n",
      "     ---> Antonov An-2V\n",
      "\n",
      "\n",
      "What was the cause of the incident that resulted in the most fatalities?\n",
      "     ---> Result - Loss of control, Security - Sabotage (bomb)\n",
      "\n",
      "\n",
      "What is the most common phase of aircraft during incidents?\n",
      "     ---> Landing (LDG)\n",
      "\n",
      "\n",
      "What is the location of the incident with the highest number of onboard occupants?\n",
      "     ---> Sioux Falls ...\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>type</th>\n",
       "      <th>columns_used</th>\n",
       "      <th>column_types</th>\n",
       "      <th>sample_answer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>float_answer</th>\n",
       "      <th>answer_coords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many unique aircraft models are in the dat...</td>\n",
       "      <td>number</td>\n",
       "      <td>['Aircaft_Model']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>20</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What's the highest number of occupants recorde...</td>\n",
       "      <td>number</td>\n",
       "      <td>['Onboard_Total']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>86</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>86.0</td>\n",
       "      <td>[(16, 3)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many incidents resulted in non-zero fatali...</td>\n",
       "      <td>number</td>\n",
       "      <td>['Fatalities']</td>\n",
       "      <td>['number[uint16]']</td>\n",
       "      <td>10</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[(0, 8), (1, 8), (9, 8), (10, 8), (11, 8), (12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which aircraft model was involved in the most ...</td>\n",
       "      <td>category</td>\n",
       "      <td>['Aircaft_Model']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Antonov An-2V</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(0, 4)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What was the cause of the incident that result...</td>\n",
       "      <td>category</td>\n",
       "      <td>['Incident_Cause(es)', 'Fatalities']</td>\n",
       "      <td>['category', 'number[uint16]']</td>\n",
       "      <td>Result - Loss of control, Security - Sabotage ...</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(9, 11)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the most common phase of aircraft duri...</td>\n",
       "      <td>category</td>\n",
       "      <td>['Aircraft_Phase']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Landing (LDG)</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(3, 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the location of the incident with the ...</td>\n",
       "      <td>category</td>\n",
       "      <td>['Incident_Location', 'Onboard_Total']</td>\n",
       "      <td>['category', 'category']</td>\n",
       "      <td>Sioux Falls ...</td>\n",
       "      <td>019_Aircraft.csv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[(16, 0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question      type  \\\n",
       "4   How many unique aircraft models are in the dat...    number   \n",
       "5   What's the highest number of occupants recorde...    number   \n",
       "7   How many incidents resulted in non-zero fatali...    number   \n",
       "8   Which aircraft model was involved in the most ...  category   \n",
       "9   What was the cause of the incident that result...  category   \n",
       "10  What is the most common phase of aircraft duri...  category   \n",
       "11  What is the location of the incident with the ...  category   \n",
       "\n",
       "                              columns_used                    column_types  \\\n",
       "4                        ['Aircaft_Model']                    ['category']   \n",
       "5                        ['Onboard_Total']                    ['category']   \n",
       "7                           ['Fatalities']              ['number[uint16]']   \n",
       "8                        ['Aircaft_Model']                    ['category']   \n",
       "9     ['Incident_Cause(es)', 'Fatalities']  ['category', 'number[uint16]']   \n",
       "10                      ['Aircraft_Phase']                    ['category']   \n",
       "11  ['Incident_Location', 'Onboard_Total']        ['category', 'category']   \n",
       "\n",
       "                                        sample_answer           dataset  \\\n",
       "4                                                  20  019_Aircraft.csv   \n",
       "5                                                  86  019_Aircraft.csv   \n",
       "7                                                  10  019_Aircraft.csv   \n",
       "8                                       Antonov An-2V  019_Aircraft.csv   \n",
       "9   Result - Loss of control, Security - Sabotage ...  019_Aircraft.csv   \n",
       "10                                      Landing (LDG)  019_Aircraft.csv   \n",
       "11                                    Sioux Falls ...  019_Aircraft.csv   \n",
       "\n",
       "    float_answer                                      answer_coords  \n",
       "4           20.0  [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...  \n",
       "5           86.0                                          [(16, 3)]  \n",
       "7           10.0  [(0, 8), (1, 8), (9, 8), (10, 8), (11, 8), (12...  \n",
       "8            NaN                                           [(0, 4)]  \n",
       "9            NaN                                          [(9, 11)]  \n",
       "10           NaN                                           [(3, 6)]  \n",
       "11           NaN                                          [(16, 0)]  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the qa \n",
    "qa_id = dfs_train[18]\n",
    "print(qa_id)\n",
    "qa = qa_dict[qa_id]\n",
    "for q, a in zip(qa['question'], qa['sample_answer']):\n",
    "    print(q)\n",
    "    print('     --->', a)\n",
    "    print('\\n')\n",
    "    \n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "86614176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 019_Aircraft\n",
    "qa_dict[dfs_train[18]]['answer_coords'] = [\n",
    "    [(0,4),(1,4),(2,4),(3,4),(4,4),(5,4),(6,4),(7,4),(8,4),(9,4),(10,4),(11,4),(12,4),(13,4),(14,4),(15,4),(16,4),(17,4),(18,4),(19,4)],\n",
    "    [(16,3)], \n",
    "    [(0,8),(1,8),(9,8),(10,8),(11,8),(12,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,4)],\n",
    "    [(9,11)], \n",
    "    [(3,6),(5,6),(7,6),(16,6),(15,6),(14,6)], #(3,6)\n",
    "    [(16,0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c917f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 018_Staff (changed)\n",
    "qa_dict[dfs_train[17]]['answer_coords'] = [\n",
    "    [(0,1),(1,1),(2,1),(3,1),(5,1),(9,1),(11,1),(12,1),(18,1)], \n",
    "    [(9,6)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,1)],\n",
    "    [(1,7),(2,7),(3,7),(4,7),(5,7),(8,7),(9,7),(11,7),(12,7),(13,7),(14,7),(17,7),(18,7)], #(1,7)\n",
    "    [(1,10)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "45c18240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 017_Hacker (changed)\n",
    "qa_dict[dfs_train[16]]['answer_coords'] = [\n",
    "    [(3,3)], \n",
    "    [(12,2)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)],\n",
    "    [(0,9),(2,9),(4,9),(11,9),(14,9),(15,9),(16,9),(18,9)],\n",
    "    [(1,6),(2,6),(9,6),(10,6),(13,6),(15,6)], # (1,6)\n",
    "    [(0,5),(1,5),(2,5),(3,5),(5,5),(6,5),(7,5),(8,5),(9,5),(10,5),(11,5),(13,5),(14,5),(16,5),(17,5),(18,5),(19,5)], # (0,5)\n",
    "    [(12,9)],\n",
    "    [(0,3),(2,3),(4,3),(5,3),(8,3),(9,3),(10,3),(11,3),(12,3),(13,3),(14,3),(15,3),(16,3)] #(0,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62154c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 016_Holiday (no changes needed)\n",
    "qa_dict[dfs_train[15]]['answer_coords'] = [\n",
    "    [(0,6),(4,6),(5,6)], \n",
    "    [(0,18),(4,18),(5,18),(10,18)], \n",
    "    [(0,12),(3,12),(9,12),(10,12)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48092367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 015_Food (no changes needed)\n",
    "qa_dict[dfs_train[14]]['answer_coords'] = [\n",
    "    [(0,2),(1,2),(2,2),(3,2),(7,2),(8,2),(9,2),(18,2)], \n",
    "    [(0,3),(1,3),(2,3),(3,3),(5,3),(6,3),(7,3),(8,3),(9,3),(12,3),(15,3),(17,3),(18,3),(19,3)],   \n",
    "    [(1,0),(2,0),(3,0),(4,0),(5,0),(6,0),(7,0),(8,0),(9,0),(10,0),(11,0),(12,0),(13,0),(14,0),(15,0),(16,0),(17,0),(18,0),(19,0),(0,0)], \n",
    "    [(1,1)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d69a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 013_Roller (no changes needed)\n",
    "qa_dict[dfs_train[12]]['answer_coords'] = [\n",
    "    [(12,8)], \n",
    "    [(3,5),(4,5),(9,5),(10,5),(11,5)], \n",
    "    [(12,3)],   \n",
    "    [(1,6)], \n",
    "    [(15,10)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d97e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 012_Heart (changed)\n",
    "qa_dict[dfs_train[11]]['answer_coords'] = [\n",
    "    [(7,1)], \n",
    "    [(17,2)],\n",
    "    [(0,0),(1,0),(4,0),(5,0),(7,0),(8,0),(10,0),(11,0),(12,0),(13,0),(14,0),(16,0),(18,0),(19,0)], \n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7),(8,7),(9,7),(10,7),(11,7),(12,7),(13,7),(14,7),(15,7),(16,7),(17,7),(18,7),(19,7)],  \n",
    "    [(2,5),(3,5),(5,5),(6,5),(9,5),(12,5),(14,5),(15,5),(17,5),(18,5)], #(2,5)\n",
    "    [(2,6),(6,6)], #(2,6)\n",
    "    [(1,4),(2,4),(3,4),(5,4),(6,4),(8,4),(10,4),(12,4),(14,4),(18,4),(19,4)], #(1,4)\n",
    "    [(8,6),(10,6)] #(8,6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "841bfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 011_SF (changes made)\n",
    "qa_dict[dfs_train[10]]['answer_coords'] = [\n",
    "    [(0,1),(2,1),(7,1),(8,1),(9,1),(10,1),(11,1),(13,1),(16,1),(18,1),(19,1)], \n",
    "    [(5,9)],\n",
    "    [(0,7),(1,7),(3,7),(4,7),(5,7),(8,7),(9,7),(10,7),(14,7)], \n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(14,8),(15,8),(16,8),(17,8),(18,8),(19,8)], # this might not work  \n",
    "    [(2,1),(4,1),(5,1),(5,1),(15,1)], #(2,1)\n",
    "    [(5,4),(8,4),(14,4),(17,4),(19,4)], #(5,4)\n",
    "    [(5,6)],\n",
    "    [(4,7),(6,7),(17,7),(16,7)] #(4,7)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51d465e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 010_ECommerce (changed)\n",
    "qa_dict[dfs_train[9]]['answer_coords'] = [\n",
    "    [(0,2),(1,2),(2,2),(3,2),(4,2),(5,2),(6,2),(7,2),(8,2),(9,2),(10,2),(11,2),(12,2),(13,2),(14,2),(15,2),(16,2),(17,2),(18,2),(19,2)], \n",
    "    [(0,0)], \n",
    "    [(4,5),(5,5),(7,5),(8,5),(9,5),(10,5),(13,5),(15,5),(17,5),(19,5)], # (4,5)\n",
    "    [(0,1),(1,1),(2,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1),(10,1),(11,1),(12,1),(13,1),(14,1),(15,1),(16,1),(17,1),(18,1),(19,1),],  \n",
    "    [(0,3),(4,3),(7,3),(10,3),(12,3),(13,3),(19,3)], # (0,3)\n",
    "    [(0,5),(4,5),(7,5),(10,5),(12,5),(13,5),(19,5)], # (0,5)\n",
    "    [(0,6),(2,6),(4,6),(7,6),(8,6),(12,6),(14,6),(15,6),(17,6),(18,6)], #(0,6)\n",
    "    [(0,1)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa819b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 009_Central (not changes needed)\n",
    "qa_dict[dfs_train[8]]['answer_coords'] = [\n",
    "    [(2,1)], \n",
    "    [(6,0)], \n",
    "    [(2,4)], \n",
    "    [(1,3)], \n",
    "    [(2,2)], \n",
    "    [(6,2)], \n",
    "    [(2,2)],\n",
    "    [(1,2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05fa8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 008_Tornadoes (changed)\n",
    "qa_dict[dfs_train[7]]['answer_coords'] = [\n",
    "    [(0,6),(1,6),(2,6),(3,6),(4,6),(7,6),(8,6),(9,6),(12,6),(15,6),(16,6),(18,6)], \n",
    "    [(10,3)],\n",
    "    [(11,7)],\n",
    "    [(16,4)],\n",
    "    [(4,6),(5,6),(6,6),(17,6)], # (4,6)\n",
    "    [(4,5),(6,5),(7,5),(11,5),(18,5)], #(6,5)\n",
    "    [(16,0)],\n",
    "    [(11,0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c131a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 007_Fifa (changed)\n",
    "qa_dict[dfs_train[6]]['answer_coords'] = [\n",
    "    [(0,11),(1,11),(2,11),(3,11),(4,11),(5,11),(6,11),(7,11),(8,11),(9,11),(10,11),(11,11),(12,11),(13,11),(14,11),(15,11),(16,11),(17,11),(18,11)], \n",
    "    [(4,10)],\n",
    "    [(11,3)],\n",
    "    [(0,8),(2,8)], #(0,8)\n",
    "    [(0,5),(1,5),(2,5),(4,5),(5,5),(6,5),(7,5),(8,5),(9,5),(10,5),(11,5),(12,5),(13,5),(16,5),(17,5)], #(0,5)\n",
    "    [(12,11),(19,11)], #(19,11)\n",
    "    [(0,3),(2,3),(4,3),(8,3),(9,3),(10,3),(12,3),(13,3),(14,3),(15,3)] #(2,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81325301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 005_NYC (changed)\n",
    "qa_dict[dfs_train[4]]['answer_coords'] = [\n",
    "    [(0,5),(1,5),(3,5),(4,5),(5,5),(11,5),(15,5)], \n",
    "    [(14,6)], # dont get\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(8,7),(9,7),(10,7),(11,7),(13,7),(14,7),(15,7),(18,7),(19,7)],\n",
    "    [(1,1),(3,1),(7,1),(9,1),(11,1),(12,1),(13,1),(14,1),(19,1)], #(1,1)\n",
    "    [(4,3),(5,3),(8,3),(9,3)], #(4,3)\n",
    "    [(17,4)],\n",
    "    [(4,5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "692ddb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 004_Taxi (changes made)\n",
    "qa_dict[dfs_train[3]]['answer_coords'] = [\n",
    "    [(2,3)], #\n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(15,8),(16,8),(18,8),(19,8)],\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7),(8,7),(9,7),(10,7),(11,7),(12,7),(13,7),(14,7),(15,7),(16,7),(17,7),(18,7),(19,7)],\n",
    "    [(0,1),(1,1),(3,1),(4,1),(5,1),(6,1),(7,1),(8,1),(9,1),(11,1),(12,1),(13,1),(15,1),(16,1),(17,1),(18,1),(19,1)], #(0,1)\n",
    "    [(1,4),(2,4),(3,4),(4,4),(6,4),(7,4),(8,4),(9,4),(10,4),(12,4),(13,4),(17,4)], # (1,4)\n",
    "    [(3,5),(4,5),(9,5)], #(3,5)\n",
    "    [(0,2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3d80820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 002_Titanic (changes made)\n",
    "qa_dict[dfs_train[1]]['answer_coords'] = [\n",
    "    [(0, 4)],\n",
    "    [(0, 0)],\n",
    "    [(0, 4),(2, 4),(3, 4),(5, 4),(6, 4),(8, 4),(12, 4),(14, 4),(15, 4),(16, 4),(18, 4)], # (0, 4)\n",
    "    [(12, 2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25933f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 001_Forbes (not changes needed)\n",
    "qa_dict[dfs_train[0]]['answer_coords'] = [\n",
    "    [(11,5)], \n",
    "    [(14,6)],\n",
    "    [(7,8)],\n",
    "    [(18,10)],\n",
    "    [(0,4)],\n",
    "    [(11,9)] \n",
    "]\n",
    "\n",
    "#qa_dict[dfs_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73bbba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up the WTQ style tokenizer\n",
    "config = TapasConfig.from_pretrained(\n",
    "    \"google/tapas-base-finetuned-wtq\",\n",
    "    aggregation_labels=True,  # Enable aggregation operators\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer and model with the configuration\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "14cbb1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "018_Staff\n"
     ]
    }
   ],
   "source": [
    "df_num = 17\n",
    "print(dfs_train[df_num])\n",
    "table = pd.read_csv(f'{dfs_train[df_num]}.csv').astype(str)\n",
    "queries = list(qa_dict[dfs_train[df_num]]['question'])\n",
    "answer_coordinates = list(qa_dict[dfs_train[df_num]]['answer_coords'])\n",
    "answer_text = list(qa_dict[dfs_train[df_num]]['sample_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0150d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:2673: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1472: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2129, 2116,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2006, 2779,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 2533,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 2095,  ...,    0,    0,    0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'numeric_values': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]), 'numeric_values_scale': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    table = table,\n",
    "    queries = queries,\n",
    "    answer_coordinates = answer_coordinates,\n",
    "    answer_text = answer_text,\n",
    "    padding = \"max_length\",\n",
    "    truncation=True,  \n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39ea4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_ids = [0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe84252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets in the working_ids to a dataset\n",
    "working_df = pd.DataFrame()\n",
    "for i in working_ids:\n",
    "    df = qa_dict[dfs_train[i]]\n",
    "    working_df = pd.concat([working_df, df], ignore_index=True)\n",
    "    \n",
    "working_df.to_csv('toy_df_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcec420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
