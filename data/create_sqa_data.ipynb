{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a836e767",
   "metadata": {},
   "source": [
    "We need to format our data into SQA format and save into a csv/tsv for the finetuning which needs:\n",
    "\n",
    "id: optional, id of the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "annotator: optional, id of the person who annotated the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "position: integer indicating if the question is the first, second, third,… related to the table. Only required in case of conversational setup (SQA). You don’t need this column in case you’re going for WTQ/WikiSQL-supervised.\n",
    "\n",
    "question: string\n",
    "\n",
    "table_file: string, name of a csv file containing the tabular data\n",
    "answer_coordinates: list of one or more tuples (each tuple being a cell coordinate, i.e. row, column pair that is part of the answer)\n",
    "\n",
    "answer_text: list of one or more strings (each string being a cell value that is part of the answer)\n",
    "aggregation_label: index of the aggregation operator. Only required in case of strong supervision for aggregation (the WikiSQL-supervised case)\n",
    "\n",
    "float_answer: the float answer to the question, if there is one (np.nan if there isn’t). Only required in case of weak supervision for aggregation (such as WTQ and WikiSQL)\n",
    "\n",
    "the tables refered to in the table_file area should be saved in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2c28430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import TapasTokenizer, TapasForQuestionAnswering, TapasConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1d509af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5f90390944f6eaeaac69b138b2614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b4213029e445f8b2719af2083475dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338e10e2fa6f41b3ba3966cfb062afdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b494658ef9c4eaba5c9f280c9c54bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in all qa (train and dev)\n",
    "semeval_train_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"train\")\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bc2d2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all of the train datasets\n",
    "dfs_train = list(set(semeval_train_qa['dataset']))\n",
    "dfs_train = sorted(dfs_train, key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# get the names of all of the dev datasets\n",
    "dfs_dev = list(set(semeval_dev_qa['dataset']))\n",
    "dfs_dev = sorted(dfs_dev, key=lambda x: int(x.split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1e14f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  001_Forbes\n",
      "CSV for ID 001_Forbes already exists. Skipping...\n",
      "Processing:  002_Titanic\n",
      "CSV for ID 002_Titanic already exists. Skipping...\n",
      "Processing:  003_Love\n",
      "CSV for ID 003_Love already exists. Skipping...\n",
      "Processing:  004_Taxi\n",
      "CSV for ID 004_Taxi already exists. Skipping...\n",
      "Processing:  005_NYC\n",
      "CSV for ID 005_NYC already exists. Skipping...\n",
      "Processing:  006_London\n",
      "CSV for ID 006_London already exists. Skipping...\n",
      "Processing:  007_Fifa\n",
      "Saved CSV for ID 007_Fifa at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/007_Fifa.csv.\n",
      "Processing:  008_Tornados\n",
      "CSV for ID 008_Tornados already exists. Skipping...\n",
      "Processing:  009_Central\n",
      "CSV for ID 009_Central already exists. Skipping...\n",
      "Processing:  010_ECommerce\n",
      "CSV for ID 010_ECommerce already exists. Skipping...\n",
      "Processing:  011_SF\n",
      "CSV for ID 011_SF already exists. Skipping...\n",
      "Processing:  012_Heart\n",
      "CSV for ID 012_Heart already exists. Skipping...\n",
      "Processing:  013_Roller\n",
      "CSV for ID 013_Roller already exists. Skipping...\n",
      "Processing:  014_Airbnb\n",
      "CSV for ID 014_Airbnb already exists. Skipping...\n",
      "Processing:  015_Food\n",
      "CSV for ID 015_Food already exists. Skipping...\n",
      "Processing:  016_Holiday\n",
      "CSV for ID 016_Holiday already exists. Skipping...\n",
      "Processing:  017_Hacker\n",
      "CSV for ID 017_Hacker already exists. Skipping...\n",
      "Processing:  018_Staff\n",
      "CSV for ID 018_Staff already exists. Skipping...\n",
      "Processing:  019_Aircraft\n",
      "CSV for ID 019_Aircraft already exists. Skipping...\n",
      "Processing:  020_Real\n",
      "CSV for ID 020_Real already exists. Skipping...\n",
      "Processing:  021_Telco\n",
      "CSV for ID 021_Telco already exists. Skipping...\n",
      "Processing:  022_Airbnbs\n",
      "CSV for ID 022_Airbnbs already exists. Skipping...\n",
      "Processing:  023_Climate\n",
      "CSV for ID 023_Climate already exists. Skipping...\n",
      "Processing:  024_Salary\n",
      "CSV for ID 024_Salary already exists. Skipping...\n",
      "Processing:  025_Data\n",
      "CSV for ID 025_Data already exists. Skipping...\n",
      "Processing:  026_Predicting\n",
      "CSV for ID 026_Predicting already exists. Skipping...\n",
      "Processing:  027_Supermarket\n",
      "CSV for ID 027_Supermarket already exists. Skipping...\n",
      "Processing:  028_Predict\n",
      "CSV for ID 028_Predict already exists. Skipping...\n",
      "Processing:  029_NYTimes\n",
      "CSV for ID 029_NYTimes already exists. Skipping...\n",
      "Processing:  030_Professionals\n",
      "CSV for ID 030_Professionals already exists. Skipping...\n",
      "Processing:  031_Trustpilot\n",
      "CSV for ID 031_Trustpilot already exists. Skipping...\n",
      "Processing:  032_Delicatessen\n",
      "CSV for ID 032_Delicatessen already exists. Skipping...\n",
      "Processing:  033_Employee\n",
      "CSV for ID 033_Employee already exists. Skipping...\n",
      "Processing:  034_World\n",
      "CSV for ID 034_World already exists. Skipping...\n",
      "Processing:  035_Billboard\n",
      "CSV for ID 035_Billboard already exists. Skipping...\n",
      "Processing:  036_US\n",
      "CSV for ID 036_US already exists. Skipping...\n",
      "Processing:  037_Ted\n",
      "CSV for ID 037_Ted already exists. Skipping...\n",
      "Processing:  038_Stroke\n",
      "CSV for ID 038_Stroke already exists. Skipping...\n",
      "Processing:  039_Happy\n",
      "CSV for ID 039_Happy already exists. Skipping...\n",
      "Processing:  040_Speed\n",
      "CSV for ID 040_Speed already exists. Skipping...\n",
      "Processing:  041_Airline\n",
      "CSV for ID 041_Airline already exists. Skipping...\n",
      "Processing:  042_Predict\n",
      "CSV for ID 042_Predict already exists. Skipping...\n",
      "Processing:  043_Predict\n",
      "CSV for ID 043_Predict already exists. Skipping...\n",
      "Processing:  044_IMDb\n",
      "CSV for ID 044_IMDb already exists. Skipping...\n",
      "Processing:  045_Predict\n",
      "CSV for ID 045_Predict already exists. Skipping...\n",
      "Processing:  046_120\n",
      "CSV for ID 046_120 already exists. Skipping...\n",
      "Processing:  047_Bank\n",
      "CSV for ID 047_Bank already exists. Skipping...\n",
      "Processing:  048_Data\n",
      "CSV for ID 048_Data already exists. Skipping...\n",
      "Processing:  049_Boris\n",
      "CSV for ID 049_Boris already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "##### load in the forbes dataframe (pandas dataframes) #####\n",
    "\n",
    "\n",
    "qa_dict = {} # dict to store all qa \n",
    "output_folder = os.getcwd()\n",
    "for table in dfs_train:\n",
    "    print('Processing: ', table)\n",
    "    csv_file_path = os.path.join(output_folder, f\"{table}.csv\")\n",
    "    \n",
    "    # Load the qa.parquet dataframe and store it in the dictionary\n",
    "    qa = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/qa.parquet\")\n",
    "    qa_dict[table] = qa\n",
    "        \n",
    "    # Skip if the CSV file already exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"CSV for ID {table} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load the all.parquet dataframe and save it as CSV\n",
    "        df = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/sample.parquet\") # loading in the lite versions with only 20 rows\n",
    "        df.to_csv(csv_file_path, index=False)  #### RERUN THIS WHEN I DO THE REAL THING\n",
    "        print(f\"Saved CSV for ID {table} at {csv_file_path}.\")\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "48667efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['001_Forbes', '002_Titanic', '003_Love', '004_Taxi', '005_NYC', '006_London', '007_Fifa', '008_Tornados', '009_Central', '010_ECommerce', '011_SF', '012_Heart', '013_Roller', '014_Airbnb', '015_Food', '016_Holiday', '017_Hacker', '018_Staff', '019_Aircraft', '020_Real', '021_Telco', '022_Airbnbs', '023_Climate', '024_Salary', '025_Data', '026_Predicting', '027_Supermarket', '028_Predict', '029_NYTimes', '030_Professionals', '031_Trustpilot', '032_Delicatessen', '033_Employee', '034_World', '035_Billboard', '036_US', '037_Ted', '038_Stroke', '039_Happy', '040_Speed', '041_Airline', '042_Predict', '043_Predict', '044_IMDb', '045_Predict', '046_120', '047_Bank', '048_Data', '049_Boris'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign all of the qa tables\n",
    "# for each need to manually assing the answer coordinate to each qa row\n",
    "qa_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "77981439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only number and category answers for all qa dfs in qa_dict\n",
    "def extract_float(answer):\n",
    "    try:\n",
    "        return float(answer)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "for df in qa_dict:\n",
    "    qa = qa_dict[df] \n",
    "    qa = qa[qa['type'].isin(['number', 'category'])] # choose only the number and category answers\n",
    "    qa = qa.drop('answer', axis = 1) # drop the answer category for the not sample dataframe\n",
    "    qa = qa.loc[~qa['sample_answer'].isin(['0', 'None'])] # filter out answer of 0 or None\n",
    "    qa['dataset'] = qa['dataset'] + '.csv'\n",
    "    #print(qa.columns)\n",
    "    qa['float_answer'] = qa['sample_answer'].apply(extract_float)\n",
    "    qa_dict[df] = qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a7cdc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other questions to remove, make sure you only run this once\n",
    "qa_dict[dfs_train[0]] = qa_dict[dfs_train[0]].iloc[:-1]\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].reset_index(drop=True)\n",
    "qa_dict[dfs_train[1]] = qa_dict[dfs_train[1]].drop([2,3,6,7])\n",
    "qa_dict[dfs_train[6]] = qa_dict[dfs_train[6]].reset_index(drop=True).drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "39d946af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many unique states are represented in the dataset?\n",
      "     ---> 12\n",
      "\n",
      "\n",
      "What is the highest magnitude of tornado recorded in the dataset?\n",
      "     ---> 2\n",
      "\n",
      "\n",
      "What is the longest length of a tornado path in the dataset?\n",
      "     ---> 72.2\n",
      "\n",
      "\n",
      "What is the maximum number of injuries caused by a single tornado?\n",
      "     ---> 3\n",
      "\n",
      "\n",
      "Which state has experienced the most tornadoes?\n",
      "     ---> IL\n",
      "\n",
      "\n",
      "In which month do most tornadoes occur?\n",
      "     ---> 6\n",
      "\n",
      "\n",
      "On what date did the most destructive tornado (by injuries) occur?\n",
      "     ---> 1973-03-15\n",
      "\n",
      "\n",
      "On what date did the longest tornado (by path length) occur?\n",
      "     ---> 1955-06-04\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>type</th>\n",
       "      <th>columns_used</th>\n",
       "      <th>column_types</th>\n",
       "      <th>sample_answer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>float_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many unique states are represented in the ...</td>\n",
       "      <td>number</td>\n",
       "      <td>[st]</td>\n",
       "      <td>['category']</td>\n",
       "      <td>12</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the highest magnitude of tornado recor...</td>\n",
       "      <td>number</td>\n",
       "      <td>[mag]</td>\n",
       "      <td>['number[int8]']</td>\n",
       "      <td>2</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the longest length of a tornado path i...</td>\n",
       "      <td>number</td>\n",
       "      <td>[len]</td>\n",
       "      <td>['number[double]']</td>\n",
       "      <td>72.2</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>72.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the maximum number of injuries caused ...</td>\n",
       "      <td>number</td>\n",
       "      <td>[inj]</td>\n",
       "      <td>['number[uint16]']</td>\n",
       "      <td>3</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which state has experienced the most tornadoes?</td>\n",
       "      <td>category</td>\n",
       "      <td>[st]</td>\n",
       "      <td>['category']</td>\n",
       "      <td>IL</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>In which month do most tornadoes occur?</td>\n",
       "      <td>category</td>\n",
       "      <td>[mo]</td>\n",
       "      <td>['number[uint8]']</td>\n",
       "      <td>6</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>On what date did the most destructive tornado ...</td>\n",
       "      <td>category</td>\n",
       "      <td>[date, inj]</td>\n",
       "      <td>['date[ns, UTC]', 'number[uint16]']</td>\n",
       "      <td>1973-03-15</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On what date did the longest tornado (by path ...</td>\n",
       "      <td>category</td>\n",
       "      <td>[date, len]</td>\n",
       "      <td>['date[ns, UTC]', 'number[double]']</td>\n",
       "      <td>1955-06-04</td>\n",
       "      <td>008_Tornados.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question      type columns_used  \\\n",
       "4   How many unique states are represented in the ...    number         [st]   \n",
       "5   What is the highest magnitude of tornado recor...    number        [mag]   \n",
       "6   What is the longest length of a tornado path i...    number        [len]   \n",
       "7   What is the maximum number of injuries caused ...    number        [inj]   \n",
       "8     Which state has experienced the most tornadoes?  category         [st]   \n",
       "9             In which month do most tornadoes occur?  category         [mo]   \n",
       "10  On what date did the most destructive tornado ...  category  [date, inj]   \n",
       "11  On what date did the longest tornado (by path ...  category  [date, len]   \n",
       "\n",
       "                           column_types sample_answer           dataset  \\\n",
       "4                          ['category']            12  008_Tornados.csv   \n",
       "5                      ['number[int8]']             2  008_Tornados.csv   \n",
       "6                    ['number[double]']          72.2  008_Tornados.csv   \n",
       "7                    ['number[uint16]']             3  008_Tornados.csv   \n",
       "8                          ['category']            IL  008_Tornados.csv   \n",
       "9                     ['number[uint8]']             6  008_Tornados.csv   \n",
       "10  ['date[ns, UTC]', 'number[uint16]']    1973-03-15  008_Tornados.csv   \n",
       "11  ['date[ns, UTC]', 'number[double]']    1955-06-04  008_Tornados.csv   \n",
       "\n",
       "    float_answer  \n",
       "4           12.0  \n",
       "5            2.0  \n",
       "6           72.2  \n",
       "7            3.0  \n",
       "8            NaN  \n",
       "9            6.0  \n",
       "10           NaN  \n",
       "11           NaN  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the qa \n",
    "qa_id = dfs_train[7]\n",
    "qa = qa_dict[qa_id]\n",
    "for q, a in zip(qa['question'], qa['sample_answer']):\n",
    "    print(q)\n",
    "    print('     --->', a)\n",
    "    print('\\n')\n",
    "    \n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05fa8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 008_Tornadoes WORKIG ON\n",
    "qa_dict[dfs_train[7]]['answer_coords'] = [\n",
    "    [(0,6),(1,6),(2,6),(3,6),(4,6),(7,6),(8,6),(9,6),(12,6),(15,6),(16,6),(18,6)], \n",
    "    [(10,3)],\n",
    "    [(11,7)],\n",
    "    [(16,4)],\n",
    "    [(4,6)], \n",
    "    [(6,5)],\n",
    "    [(16,0)],\n",
    "    [(11,0)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c131a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 007_Fifa\n",
    "qa_dict[dfs_train[6]]['answer_coords'] = [\n",
    "    [(0,11),(1,11),(2,11),(3,11),(4,11),(5,11),(6,11),(7,11),(8,11),(9,11),(10,11),(11,11),(12,11),(13,11),(14,11),(15,11),(16,11),(17,11),(18,11)], \n",
    "    [(4,10)],\n",
    "    [(11,3)],\n",
    "    [(0,8)],\n",
    "    [(0,5)],\n",
    "    [(19,11)],\n",
    "    [(2,3)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "81325301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 005_NYC\n",
    "qa_dict[dfs_train[4]]['answer_coords'] = [\n",
    "    [(0,5),(1,5),(3,5),(4,5),(5,5),(11,5),(15,5)], \n",
    "    [(14,6)], # dont get\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(8,7),(9,7),(10,7),(11,7),(13,7),(14,7),(15,7),(18,7),(19,7)],\n",
    "    [(1,1)],\n",
    "    [(4,3)],\n",
    "    [(17,4)],\n",
    "    [(4,5)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "692ddb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 004_Taxi\n",
    "qa_dict[dfs_train[3]]['answer_coords'] = [\n",
    "    [(2,3)], #\n",
    "    [(0,8),(1,8),(2,8),(3,8),(4,8),(5,8),(6,8),(7,8),(8,8),(9,8),(10,8),(11,8),(12,8),(13,8),(15,8),(16,8),(18,8),(19,8)],\n",
    "    [(0,7),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7),(8,7),(9,7),(10,7),(11,7),(12,7),(13,7),(14,7),(15,7),(16,7),(17,7),(18,7),(19,7)],\n",
    "    [(1,1)],\n",
    "    [(1,4)],\n",
    "    [(3,5)],\n",
    "    [(0,2)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "25933f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# works for 001_Forbes \n",
    "qa_dict[dfs_train[0]]['answer_coords'] = [\n",
    "    [(11,5)], \n",
    "    [(14,6)],\n",
    "    [(7,8)],\n",
    "    [(18,10)],\n",
    "    [(0,4)],\n",
    "    [(11,9)] \n",
    "]\n",
    "\n",
    "#qa_dict[dfs_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a72df4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on 002_Titanic --> not working\n",
    "qa_dict[dfs_train[1]]['answer_coords'] = [\n",
    "    [(0,4)], # this is a fudge\n",
    "    [(0,0)],\n",
    "    [(0,4)],\n",
    "    [(12,2)] \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "73bbba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up the WTQ style tokenizer\n",
    "config = TapasConfig.from_pretrained(\n",
    "    \"google/tapas-base-finetuned-wtq\",\n",
    "    aggregation_labels=True,  # Enable aggregation operators\n",
    ")\n",
    "\n",
    "# Initialize the tokenizer and model with the configuration\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14cbb1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_Forbes\n"
     ]
    }
   ],
   "source": [
    "df_num = 0\n",
    "print(dfs_train[df_num])\n",
    "table = pd.read_csv(f'{dfs_train[df_num]}.csv').astype(str)\n",
    "queries = list(qa_dict[dfs_train[df_num]]['question'])\n",
    "answer_coordinates = list(qa_dict[dfs_train[df_num]]['answer_coords'])\n",
    "answer_text = list(qa_dict[dfs_train[df_num]]['sample_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0150d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:2673: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1472: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2054, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2029, 4696,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 1005,  ...,    0,    0,    0]]), 'labels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'numeric_values': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]]), 'numeric_values_scale': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    table = table,\n",
    "    queries = queries,\n",
    "    answer_coordinates = answer_coordinates,\n",
    "    answer_text = answer_text,\n",
    "    padding = \"max_length\",\n",
    "    truncation=True,  \n",
    "    return_tensors = \"pt\"\n",
    ")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "39ea4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_ids = [0, 1, 3, 4, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe84252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets in the working_ids to a dataset\n",
    "working_df = pd.DataFrame()\n",
    "for i in working_ids:\n",
    "    df = qa_dict[dfs_train[i]]\n",
    "    working_df = pd.concat([working_df, df], ignore_index=True)\n",
    "    \n",
    "working_df.to_csv('toy_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcec420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
