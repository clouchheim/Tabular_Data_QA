{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a836e767",
   "metadata": {},
   "source": [
    "We need to format our data into SQA format and save into a csv/tsv for the finetuning which needs:\n",
    "\n",
    "id: optional, id of the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "annotator: optional, id of the person who annotated the table-question pair, for bookkeeping purposes.\n",
    "\n",
    "position: integer indicating if the question is the first, second, third,‚Ä¶ related to the table. Only required in case of conversational setup (SQA). You don‚Äôt need this column in case you‚Äôre going for WTQ/WikiSQL-supervised.\n",
    "\n",
    "question: string\n",
    "\n",
    "table_file: string, name of a csv file containing the tabular data\n",
    "answer_coordinates: list of one or more tuples (each tuple being a cell coordinate, i.e. row, column pair that is part of the answer)\n",
    "\n",
    "answer_text: list of one or more strings (each string being a cell value that is part of the answer)\n",
    "aggregation_label: index of the aggregation operator. Only required in case of strong supervision for aggregation (the WikiSQL-supervised case)\n",
    "\n",
    "float_answer: the float answer to the question, if there is one (np.nan if there isn‚Äôt). Only required in case of weak supervision for aggregation (such as WTQ and WikiSQL)\n",
    "\n",
    "the tables refered to in the table_file area should be saved in a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c28430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import TapasTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d509af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af27bea956a24ee085db48c424400092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90da6e13300e41a19fc0b509527d555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e4ce2e68f541af83cec1a98d28280a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e559eb09a14058b7772c972036bedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in all qa (train and dev)\n",
    "semeval_train_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"train\")\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e14f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  003_Love\n",
      "Saved CSV for ID 003_Love at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/003_Love.csv.\n",
      "Processing:  025_Data\n",
      "Saved CSV for ID 025_Data at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/025_Data.csv.\n",
      "Processing:  034_World\n",
      "Saved CSV for ID 034_World at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/034_World.csv.\n",
      "Processing:  042_Predict\n",
      "Saved CSV for ID 042_Predict at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/042_Predict.csv.\n",
      "Processing:  054_Joe\n",
      "Saved CSV for ID 054_Joe at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/054_Joe.csv.\n",
      "Processing:  056_Emoji\n",
      "Saved CSV for ID 056_Emoji at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/056_Emoji.csv.\n",
      "Processing:  064_Clustering\n",
      "Saved CSV for ID 064_Clustering at /Users/carterlouchheim/Desktop/CS375/final/Tabular_Data_QA/data/064_Clustering.csv.\n"
     ]
    }
   ],
   "source": [
    "# RERUN THIS LATER TO REMOVE INDEX, JUST NEED FOR MANUAL ANSWER COORDS\n",
    "\n",
    "##### load in the forbes dataframe (pandas dataframes) #####\n",
    "df_ids = ['003_Love',\n",
    "        '025_Data',\n",
    "        '034_World', \n",
    "        '042_Predict',\n",
    "        '054_Joe',\n",
    "        '056_Emoji',\n",
    "        '064_Clustering'\n",
    "        ] # these are the ids of the dataframes that have under 512 rows\n",
    "\n",
    "qa_dict = {} # dict to store all qa \n",
    "output_folder = os.getcwd()\n",
    "for table in df_ids:\n",
    "    print('Processing: ', table)\n",
    "    csv_file_path = os.path.join(output_folder, f\"{table}.csv\")\n",
    "\n",
    "    # Skip if the CSV file already exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"CSV for ID {table} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load the all.parquet dataframe and save it as CSV\n",
    "        df = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/all.parquet\")\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Saved CSV for ID {table} at {csv_file_path}.\")\n",
    "\n",
    "        # Load the qa.parquet dataframe and store it in the dictionary\n",
    "        qa = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{table}/qa.parquet\")\n",
    "        qa_dict[table] = qa\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {table}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660124a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggretation operators\n",
    "aggregartion_ops = ['SUM', 'COUNT', 'AVERAGE', 'NONE'] # it would be hard to add new tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48667efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign all of the qa tables\n",
    "# for each need to manually assing the answer coordinate to each qa row\n",
    "love_qa = qa_dict[df_ids[0]] # DONE\n",
    "data_qa = qa_dict[df_ids[1]]\n",
    "world_qa = qa_dict[df_ids[2]]\n",
    "predict_qa = qa_dict[df_ids[3]]\n",
    "joe_qa = qa_dict[df_ids[4]]\n",
    "emoji_qa = qa_dict[df_ids[5]]\n",
    "clustering_qa = qa_dict[df_ids[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77981439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "      <th>columns_used</th>\n",
       "      <th>column_types</th>\n",
       "      <th>sample_answer</th>\n",
       "      <th>dataset</th>\n",
       "      <th>answer_coords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many unique nationalities are present in t...</td>\n",
       "      <td>13</td>\n",
       "      <td>number</td>\n",
       "      <td>[What's your nationality?\"]\"</td>\n",
       "      <td>['category']</td>\n",
       "      <td>1</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the average gross annual salary?</td>\n",
       "      <td>56332.81720430108</td>\n",
       "      <td>number</td>\n",
       "      <td>['Gross annual salary (in euros) üí∏']</td>\n",
       "      <td>['number[UInt32]']</td>\n",
       "      <td>62710.0</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many respondents wear glasses all the time?</td>\n",
       "      <td>98</td>\n",
       "      <td>number</td>\n",
       "      <td>['How often do you wear glasses? üëì']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>5</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(16, 0), (16, 2), (16, 3), (16, 5), (16, 6), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the median age of the respondents?</td>\n",
       "      <td>33.0</td>\n",
       "      <td>number</td>\n",
       "      <td>['What is your age? üë∂üèªüëµüèª']</td>\n",
       "      <td>['number[uint8]']</td>\n",
       "      <td>32.5</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the most common level of studies achie...</td>\n",
       "      <td>Master</td>\n",
       "      <td>category</td>\n",
       "      <td>['What is the maximum level of studies you hav...</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Master</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Which body complexity has the least number of ...</td>\n",
       "      <td>Very thin</td>\n",
       "      <td>category</td>\n",
       "      <td>['What is your body complexity? üèãÔ∏è']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Obese</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(10, 0), (10, 1), (10, 2), (10, 3), (10, 4), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What's the most frequent eye color?</td>\n",
       "      <td>Brown</td>\n",
       "      <td>category</td>\n",
       "      <td>['What is your eye color? üëÅÔ∏è']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Brown</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(11, 0), (11, 1), (11, 2), (11, 3), (11, 4), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Which sexual orientation has the highest repre...</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>category</td>\n",
       "      <td>['What's your sexual orientation?']</td>\n",
       "      <td>['category']</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>003_Love</td>\n",
       "      <td>[(4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question             answer  \\\n",
       "4   How many unique nationalities are present in t...                 13   \n",
       "5            What is the average gross annual salary?  56332.81720430108   \n",
       "6     How many respondents wear glasses all the time?                 98   \n",
       "7           What's the median age of the respondents?               33.0   \n",
       "8   What is the most common level of studies achie...             Master   \n",
       "9   Which body complexity has the least number of ...          Very thin   \n",
       "10                What's the most frequent eye color?              Brown   \n",
       "11  Which sexual orientation has the highest repre...       Heterosexual   \n",
       "\n",
       "        type                                       columns_used  \\\n",
       "4     number                       [What's your nationality?\"]\"   \n",
       "5     number               ['Gross annual salary (in euros) üí∏']   \n",
       "6     number               ['How often do you wear glasses? üëì']   \n",
       "7     number                         ['What is your age? üë∂üèªüëµüèª']   \n",
       "8   category  ['What is the maximum level of studies you hav...   \n",
       "9   category               ['What is your body complexity? üèãÔ∏è']   \n",
       "10  category                     ['What is your eye color? üëÅÔ∏è']   \n",
       "11  category                ['What's your sexual orientation?']   \n",
       "\n",
       "          column_types sample_answer   dataset  \\\n",
       "4         ['category']             1  003_Love   \n",
       "5   ['number[UInt32]']       62710.0  003_Love   \n",
       "6         ['category']             5  003_Love   \n",
       "7    ['number[uint8]']          32.5  003_Love   \n",
       "8         ['category']        Master  003_Love   \n",
       "9         ['category']         Obese  003_Love   \n",
       "10        ['category']         Brown  003_Love   \n",
       "11        ['category']  Heterosexual  003_Love   \n",
       "\n",
       "                                        answer_coords  \n",
       "4   [(2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5...  \n",
       "5   [(7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5...  \n",
       "6   [(16, 0), (16, 2), (16, 3), (16, 5), (16, 6), ...  \n",
       "7   [(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5...  \n",
       "8   [(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5...  \n",
       "9   [(10, 0), (10, 1), (10, 2), (10, 3), (10, 4), ...  \n",
       "10  [(11, 0), (11, 1), (11, 2), (11, 3), (11, 4), ...  \n",
       "11  [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love_qa = love_qa[love_qa['type'] != 'boolean' ]\n",
    "love_qa = love_qa[love_qa['type'] != 'list[category]' ]\n",
    "love_qa = love_qa[love_qa['type'] != 'list[number]' ]\n",
    "love_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25933f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "love_df = pd.read_csv(f\"{df_ids[0]}.csv\")\n",
    "all_rows = list(range(len(love_df))) # use when the answer attends to all rows in the\n",
    "love_ans_coords = []\n",
    "love_float_ans = [] # maybe have to add this\n",
    "\n",
    "'''# QA 0\n",
    "qa_0 = [(row, 1) for row in all_rows]\n",
    "love_ans_coords.append(qa_0)\n",
    "\n",
    "# QA 1\n",
    "qa_1 = [(row, 3) for row in all_rows]\n",
    "love_ans_coords.append(qa_1)\n",
    "\n",
    "# QA 2\n",
    "qa_2 = [(row, 8) for row in all_rows]\n",
    "love_ans_coords.append(qa_2)\n",
    "\n",
    "# QA 3\n",
    "qa_3 = [(row, 12) for row in all_rows]\n",
    "love_ans_coords.append(qa_3)'''\n",
    "\n",
    "# QA 4\n",
    "qa_4 = [(row, 2) for row in all_rows]\n",
    "love_ans_coords.append(qa_4)\n",
    "\n",
    "# QA 5\n",
    "qa_5 = [(row, 7) for row in all_rows]\n",
    "love_ans_coords.append(qa_5)\n",
    "\n",
    "# QA 6\n",
    "row_indices = love_df.index[love_df['How often do you wear glasses? üëì'] == 'Constantly'].tolist()\n",
    "qa_6 = [(row, 16) for row in row_indices]\n",
    "love_ans_coords.append(qa_6)\n",
    "\n",
    "# QA 7\n",
    "qa_7 = [(row, 1) for row in all_rows]\n",
    "love_ans_coords.append(qa_7)\n",
    "\n",
    "# QA 8\n",
    "qa_8 = [(row, 1) for row in all_rows]\n",
    "love_ans_coords.append(qa_8)\n",
    "\n",
    "# QA 9\n",
    "qa_9 = [(row, 10) for row in all_rows]\n",
    "love_ans_coords.append(qa_9)\n",
    "\n",
    "# QA 10\n",
    "qa_10 = [(row, 11) for row in all_rows]\n",
    "love_ans_coords.append(qa_10)\n",
    "\n",
    "# QA 11\n",
    "qa_11 = [(row, 4) for row in all_rows]\n",
    "love_ans_coords.append(qa_11)\n",
    "\n",
    "'''# QA 12\n",
    "qa_12 = [(row, 33) for row in all_rows]\n",
    "love_ans_coords.append(qa_12)\n",
    "\n",
    "# QA 13\n",
    "qa_13 = [(row, 14) for row in all_rows]\n",
    "love_ans_coords.append(qa_13)\n",
    "\n",
    "# QA 14\n",
    "qa_14 = [(row, 3) for row in all_rows]\n",
    "love_ans_coords.append(qa_14)\n",
    "\n",
    "# QA 15\n",
    "qa_15 = [(row, 12) for row in all_rows]\n",
    "love_ans_coords.append(qa_15)\n",
    "\n",
    "# QA 16\n",
    "row_indicies = love_df['Gross annual salary (in euros) üí∏'].nlargest(4).index.tolist()\n",
    "qa_16 = [(row, 7) for row in row_indicies]\n",
    "love_ans_coords.append(qa_16)\n",
    "\n",
    "# QA 17\n",
    "row_indicies = love_df['Happiness scale'].nsmallest(3).index.tolist()\n",
    "qa_17 = [(row, 32) for row in row_indicies]\n",
    "love_ans_coords.append(qa_17)\n",
    "\n",
    "# QA 18\n",
    "row_indicies = love_df['What is your age? üë∂üèªüëµüèª'].nlargest(5).index.tolist()\n",
    "qa_18 = [(row, 1) for row in row_indicies]\n",
    "love_ans_coords.append(qa_18)\n",
    "\n",
    "# QA 19\n",
    "row_indicies = love_df['What is your skin tone?'].nlargest(5).index.tolist()\n",
    "qa_19 = [(row, 13) for row in row_indicies]\n",
    "love_ans_coords.append(qa_19)'''\n",
    "\n",
    "# add these to a dataframe\n",
    "love_qa['answer_coords'] = love_ans_coords\n",
    "qa_dict[df_ids[0]] = love_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data qa manual\n",
    "all_rows = list(range(len(pd.read_csv(f\"{df_ids[1]}.csv\")))) # use when the answer attends to all rows in the \n",
    "# QA 0\n",
    "# QA 1\n",
    "# QA 2\n",
    "# QA 3\n",
    "# QA 4\n",
    "# QA 5\n",
    "# QA 6\n",
    "# QA 7\n",
    "# QA 8\n",
    "# QA 9\n",
    "# QA 10\n",
    "# QA 11\n",
    "# QA 12\n",
    "# QA 13\n",
    "# QA 14\n",
    "# QA 15\n",
    "# QA 16\n",
    "# QA 17\n",
    "# QA 18\n",
    "# QA 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2f80f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# world qa manual\n",
    "all_rows = list(range(len(pd.read_csv(f\"{df_ids[2]}.csv\")))) # use when the answer attends to all rows in the \n",
    "# QA 0\n",
    "# QA 1\n",
    "# QA 2\n",
    "# QA 3\n",
    "# QA 4\n",
    "# QA 5\n",
    "# QA 6\n",
    "# QA 7\n",
    "# QA 8\n",
    "# QA 9\n",
    "# QA 10\n",
    "# QA 11\n",
    "# QA 12\n",
    "# QA 13\n",
    "# QA 14\n",
    "# QA 15\n",
    "# QA 16\n",
    "# QA 17\n",
    "# QA 18\n",
    "# QA 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38b8b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predcit qa manual\n",
    "all_rows = list(range(len(pd.read_csv(f\"{df_ids[3]}.csv\")))) # use when the answer attends to all rows in the \n",
    "# QA 0\n",
    "# QA 1\n",
    "# QA 2\n",
    "# QA 3\n",
    "# QA 4\n",
    "# QA 5\n",
    "# QA 6\n",
    "# QA 7\n",
    "# QA 8\n",
    "# QA 9\n",
    "# QA 10\n",
    "# QA 11\n",
    "# QA 12\n",
    "# QA 13\n",
    "# QA 14\n",
    "# QA 15\n",
    "# QA 16\n",
    "# QA 17\n",
    "# QA 18\n",
    "# QA 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e67939e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# example of how to load in model and then format the data\n",
    "model_name = \"google/tapas-base\"\n",
    "tokenizer = TapasTokenizer.from_pretrained(model_name)\n",
    "\n",
    "test = qa_dict[df_ids[0]]\n",
    "\n",
    "table = love_df.astype(str)\n",
    "queries = list(test['question'])\n",
    "answer_coords = list(test['answer_coords'])\n",
    "answer_text = list(test['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0150d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:2673: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1472: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't find all answers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      2\u001b[0m     table \u001b[38;5;241m=\u001b[39m table,\n\u001b[1;32m      3\u001b[0m     queries \u001b[38;5;241m=\u001b[39m queries,\n\u001b[1;32m      4\u001b[0m     answer_coordinates \u001b[38;5;241m=\u001b[39m answer_coords,\n\u001b[1;32m      5\u001b[0m     answer_text \u001b[38;5;241m=\u001b[39m answer_text,\n\u001b[1;32m      6\u001b[0m     padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      7\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[1;32m      8\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:574\u001b[0m, in \u001b[0;36mTapasTokenizer.__call__\u001b[0;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(queries, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m    575\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[1;32m    576\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m    577\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[1;32m    578\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[1;32m    579\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    580\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    581\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    582\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    583\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    584\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    585\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    586\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    587\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    588\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    589\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    590\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    591\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m    596\u001b[0m         table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[1;32m    597\u001b[0m         query\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    614\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:692\u001b[0m, in \u001b[0;36mTapasTokenizer.batch_encode_plus\u001b[0;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m     )\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    693\u001b[0m     table\u001b[38;5;241m=\u001b[39mtable,\n\u001b[1;32m    694\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m    695\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[1;32m    696\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[1;32m    697\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    698\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    699\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    700\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    701\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    702\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    703\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    704\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    705\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    706\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    707\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    708\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    709\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    711\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:759\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_encode_plus\u001b[0;34m(self, table, queries, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m     queries[idx] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m    757\u001b[0m     queries_tokens\u001b[38;5;241m.\u001b[39mappend(query_tokens)\n\u001b[0;32m--> 759\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_prepare_for_model(\n\u001b[1;32m    760\u001b[0m     table,\n\u001b[1;32m    761\u001b[0m     queries,\n\u001b[1;32m    762\u001b[0m     tokenized_table\u001b[38;5;241m=\u001b[39mtable_tokens,\n\u001b[1;32m    763\u001b[0m     queries_tokens\u001b[38;5;241m=\u001b[39mqueries_tokens,\n\u001b[1;32m    764\u001b[0m     answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates,\n\u001b[1;32m    765\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    766\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    767\u001b[0m     answer_text\u001b[38;5;241m=\u001b[39manswer_text,\n\u001b[1;32m    768\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    769\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    770\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    771\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    772\u001b[0m     prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    774\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    775\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    776\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    777\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    778\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:814\u001b[0m, in \u001b[0;36mTapasTokenizer._batch_prepare_for_model\u001b[0;34m(self, raw_table, raw_queries, tokenized_table, queries_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(raw_queries, queries_tokens, answer_coordinates, answer_text)):\n\u001b[1;32m    813\u001b[0m     raw_query, query_tokens, answer_coords, answer_txt \u001b[38;5;241m=\u001b[39m example\n\u001b[0;32m--> 814\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    815\u001b[0m         raw_table,\n\u001b[1;32m    816\u001b[0m         raw_query,\n\u001b[1;32m    817\u001b[0m         tokenized_table\u001b[38;5;241m=\u001b[39mtokenized_table,\n\u001b[1;32m    818\u001b[0m         query_tokens\u001b[38;5;241m=\u001b[39mquery_tokens,\n\u001b[1;32m    819\u001b[0m         answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coords,\n\u001b[1;32m    820\u001b[0m         answer_text\u001b[38;5;241m=\u001b[39manswer_txt,\n\u001b[1;32m    821\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    822\u001b[0m         padding\u001b[38;5;241m=\u001b[39mPaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD\u001b[38;5;241m.\u001b[39mvalue,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m    824\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    825\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# we pad in batch afterwards\u001b[39;00m\n\u001b[1;32m    827\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    828\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    829\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    830\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[1;32m    831\u001b[0m         prepend_batch_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    832\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    833\u001b[0m         prev_answer_coordinates\u001b[38;5;241m=\u001b[39manswer_coordinates[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    834\u001b[0m         prev_answer_text\u001b[38;5;241m=\u001b[39manswer_text[index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m batch_outputs:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1172\u001b[0m, in \u001b[0;36mTapasTokenizer.prepare_for_model\u001b[0;34m(self, raw_table, raw_query, tokenized_table, query_tokens, answer_coordinates, answer_text, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer_coordinates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m answer_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1172\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_answer_ids(column_ids, row_ids, table_data, answer_text, answer_coordinates)\n\u001b[1;32m   1173\u001b[0m     numeric_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_numeric_values(raw_table, column_ids, row_ids)\n\u001b[1;32m   1174\u001b[0m     numeric_values_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_numeric_values_scale(raw_table, column_ids, row_ids)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1749\u001b[0m, in \u001b[0;36mTapasTokenizer.get_answer_ids\u001b[0;34m(self, column_ids, row_ids, tokenized_table, answer_texts_question, answer_coordinates_question)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_answer_coordinates:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_answer_ids_from_answer_texts(\n\u001b[1;32m   1744\u001b[0m         column_ids,\n\u001b[1;32m   1745\u001b[0m         row_ids,\n\u001b[1;32m   1746\u001b[0m         tokenized_table,\n\u001b[1;32m   1747\u001b[0m         answer_texts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(at) \u001b[38;5;28;01mfor\u001b[39;00m at \u001b[38;5;129;01min\u001b[39;00m answer_texts_question],\n\u001b[1;32m   1748\u001b[0m     )\n\u001b[0;32m-> 1749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_answer_ids(column_ids, row_ids, answer_coordinates_question)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1738\u001b[0m, in \u001b[0;36mTapasTokenizer._get_answer_ids\u001b[0;34m(self, column_ids, row_ids, answer_coordinates)\u001b[0m\n\u001b[1;32m   1735\u001b[0m answer_ids, missing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_answer_ids(column_ids, row_ids, answer_coordinates)\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_count:\n\u001b[0;32m-> 1738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find all answers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer_ids\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't find all answers"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    table = table,\n",
    "    queries = queries,\n",
    "    answer_coordinates = answer_coords,\n",
    "    answer_text = answer_text,\n",
    "    padding = 2048,\n",
    "    truncation=True,  \n",
    "    return_tensors = \"pt\",\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3b16698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TapasTokenizer\n",
    "\n",
    "class DebugTapasTokenizer(TapasTokenizer):\n",
    "    def _get_all_answer_ids(self, column_ids, row_ids, answer_coordinates):\n",
    "        answer_ids = []\n",
    "        missing_count = 0\n",
    "        missing_answers = []\n",
    "\n",
    "        for col_idx, row_idx in answer_coordinates:\n",
    "            match_found = False\n",
    "            for idx, (col_id, row_id) in enumerate(zip(column_ids, row_ids)):\n",
    "                if col_id == col_idx and row_id == row_idx:\n",
    "                    answer_ids.append(idx)\n",
    "                    match_found = True\n",
    "                    break\n",
    "            \n",
    "            if not match_found:\n",
    "                missing_count += 1\n",
    "                missing_answers.append((col_idx, row_idx))\n",
    "\n",
    "        if missing_count > 0:\n",
    "            print(f\"Missing answers: {missing_answers}\")\n",
    "        return answer_ids, missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b38124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc06bc9df944094aace9d93fc0b46f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/490 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318e21a42e5f4fdf847469f5a05dd5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/262k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215de24bede2490dbe4c97a0ed2d3e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not TapasTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m TapasTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/tapas-base-finetuned-wtq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m debug_tokenizer \u001b[38;5;241m=\u001b[39m DebugTapasTokenizer(tokenizer)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs375/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:266\u001b[0m, in \u001b[0;36mTapasTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, empty_token, tokenize_chinese_chars, strip_accents, cell_trim_length, max_column_id, max_row_id, strip_column_names, update_answer_coordinates, min_question_length, max_question_length, model_max_length, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     additional_special_tokens \u001b[38;5;241m=\u001b[39m [empty_token]\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocab_file):\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[0;32m<frozen genericpath>:30\u001b[0m, in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not TapasTokenizer"
     ]
    }
   ],
   "source": [
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
    "debug_tokenizer = DebugTapasTokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777076a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# things to try\n",
    "# fudge the asnwer coords on the majority stuff and just give the link to a cell that is majority \n",
    "# ensure that the answer coords are in the right form and give the right value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
